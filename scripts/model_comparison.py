# DESAFIO II - ComparaÃ§Ã£o de Modelos de Machine Learning
# ========================================================
# Script para comparaÃ§Ã£o de mÃºltiplos modelos preditivos

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# ==========================
# 1. CARREGAMENTO DOS DADOS
# ==========================

print("Carregando dados...")
df = pd.read_csv('/home/alencaravelar/Desktop/zetta-lab/zetta-lab2/zetta-lab2/data/refined/base_udh_refined.csv')

# Definir features e target
X = df.drop(columns=['IDHM'])
y = df['IDHM']

# DivisÃ£o treino/teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Dados carregados: {X.shape[0]} observaÃ§Ãµes, {X.shape[1]} features")
print(f"Treino: {X_train.shape[0]} | Teste: {X_test.shape[0]}\n")

# ================================
# 2. DEFINIÃ‡ÃƒO DOS MODELOS
# ================================

models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(random_state=42),
    'Lasso': Lasso(random_state=42),
    'ElasticNet': ElasticNet(random_state=42),
    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),
    'Random Forest': RandomForestRegressor(
        n_estimators=200, max_depth=20, random_state=42
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42
    ),
    'AdaBoost': AdaBoostRegressor(random_state=42, n_estimators=100),
    'KNN': KNeighborsRegressor(n_neighbors=5),
    'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.01)
}

# =================================
# 3. TREINAMENTO E AVALIAÃ‡ÃƒO
# =================================

results = []

print("="*80)
print("TREINAMENTO E AVALIAÃ‡ÃƒO DOS MODELOS")
print("="*80 + "\n")

for name, model in models.items():
    print(f"Treinando {name}...")
    
    # Treinar modelo
    model.fit(X_train, y_train)
    
    # PrevisÃµes
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # MÃ©tricas
    r2_train = r2_score(y_train, y_train_pred)
    r2_test = r2_score(y_test, y_test_pred)
    mae = mean_absolute_error(y_test, y_test_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    results.append({
        'Model': name,
        'RÂ² Train': r2_train,
        'RÂ² Test': r2_test,
        'MAE': mae,
        'RMSE': rmse,
        'CV RÂ² Mean': cv_mean,
        'CV RÂ² Std': cv_std,
        'Overfitting': r2_train - r2_test
    })
    
    print(f"  RÂ² Test: {r2_test:.6f} | MAE: {mae:.6f} | RMSE: {rmse:.6f}\n")

# ====================================
# 4. CRIAR DATAFRAME DE RESULTADOS
# ====================================

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('RÂ² Test', ascending=False).reset_index(drop=True)

print("\n" + "="*80)
print("RESULTADOS COMPARATIVOS (Ordenados por RÂ² Test)")
print("="*80)
print(results_df.to_string(index=False))

# Salvar resultados
results_df.to_csv('model_comparison_results.csv', index=False)
print("\nâœ… Resultados salvos em 'model_comparison_results.csv'")

# ====================================
# 5. VISUALIZAÃ‡Ã•ES
# ====================================

# Configurar estilo
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (14, 10)

# 5.1 ComparaÃ§Ã£o de RÂ² Test
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# GrÃ¡fico 1: RÂ² Test
ax1 = axes[0, 0]
colors = sns.color_palette('viridis', len(results_df))
results_df.plot(kind='barh', x='Model', y='RÂ² Test', ax=ax1, color=colors, legend=False)
ax1.set_xlabel('RÂ² Test', fontsize=12, fontweight='bold')
ax1.set_ylabel('Modelo', fontsize=12, fontweight='bold')
ax1.set_title('ComparaÃ§Ã£o: RÂ² Test', fontsize=14, fontweight='bold')
ax1.axvline(x=results_df['RÂ² Test'].mean(), color='red', linestyle='--', label='MÃ©dia')
ax1.legend()

# GrÃ¡fico 2: MAE
ax2 = axes[0, 1]
results_df.plot(kind='barh', x='Model', y='MAE', ax=ax2, color=colors, legend=False)
ax2.set_xlabel('Mean Absolute Error', fontsize=12, fontweight='bold')
ax2.set_ylabel('')
ax2.set_title('ComparaÃ§Ã£o: MAE (menor Ã© melhor)', fontsize=14, fontweight='bold')

# GrÃ¡fico 3: RMSE
ax3 = axes[1, 0]
results_df.plot(kind='barh', x='Model', y='RMSE', ax=ax3, color=colors, legend=False)
ax3.set_xlabel('Root Mean Squared Error', fontsize=12, fontweight='bold')
ax3.set_ylabel('Modelo', fontsize=12, fontweight='bold')
ax3.set_title('ComparaÃ§Ã£o: RMSE (menor Ã© melhor)', fontsize=14, fontweight='bold')

# GrÃ¡fico 4: Overfitting (diferenÃ§a RÂ² Train - Test)
ax4 = axes[1, 1]
results_df.plot(kind='barh', x='Model', y='Overfitting', ax=ax4, color=colors, legend=False)
ax4.set_xlabel('Overfitting (RÂ² Train - RÂ² Test)', fontsize=12, fontweight='bold')
ax4.set_ylabel('')
ax4.set_title('AnÃ¡lise de Overfitting (menor Ã© melhor)', fontsize=14, fontweight='bold')
ax4.axvline(x=0.01, color='orange', linestyle='--', label='Threshold 0.01')
ax4.legend()

plt.tight_layout()
plt.savefig('model_comparison_metrics.png', dpi=300, bbox_inches='tight')
print("âœ… GrÃ¡fico de comparaÃ§Ã£o salvo em 'model_comparison_metrics.png'")
plt.close()

# 5.2 VisualizaÃ§Ã£o de RÂ² Train vs Test
plt.figure(figsize=(12, 8))
x = np.arange(len(results_df))
width = 0.35

plt.barh(x - width/2, results_df['RÂ² Train'], width, label='RÂ² Train', alpha=0.8)
plt.barh(x + width/2, results_df['RÂ² Test'], width, label='RÂ² Test', alpha=0.8)

plt.ylabel('Modelo', fontweight='bold')
plt.xlabel('RÂ² Score', fontweight='bold')
plt.title('RÂ² Train vs RÂ² Test - AnÃ¡lise de GeneralizaÃ§Ã£o', fontsize=14, fontweight='bold')
plt.yticks(x, results_df['Model'])
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('model_r2_train_vs_test.png', dpi=300, bbox_inches='tight')
print("âœ… GrÃ¡fico RÂ² Train vs Test salvo em 'model_r2_train_vs_test.png'")
plt.close()

# ====================================
# 6. ANÃLISE E RECOMENDAÃ‡Ã•ES
# ====================================

print("\n" + "="*80)
print("ANÃLISE E RECOMENDAÃ‡Ã•ES")
print("="*80)

best_model = results_df.iloc[0]
print(f"\nðŸ† MELHOR MODELO: {best_model['Model']}")
print(f"   - RÂ² Test: {best_model['RÂ² Test']:.6f}")
print(f"   - MAE: {best_model['MAE']:.6f}")
print(f"   - RMSE: {best_model['RMSE']:.6f}")
print(f"   - Overfitting: {best_model['Overfitting']:.6f}")

print("\nðŸ“Š TOP 3 MODELOS:")
for i, row in results_df.head(3).iterrows():
    print(f"{i+1}. {row['Model']} - RÂ² Test: {row['RÂ² Test']:.6f}")

print("\nâš ï¸  MODELOS COM OVERFITTING SIGNIFICATIVO (>0.05):")
overfitting_models = results_df[results_df['Overfitting'] > 0.05]
if len(overfitting_models) > 0:
    for _, row in overfitting_models.iterrows():
        print(f"   - {row['Model']}: {row['Overfitting']:.6f}")
else:
    print("   âœ… Nenhum modelo apresentou overfitting significativo")

print("\n" + "="*80)
print("SCRIPT FINALIZADO COM SUCESSO!")
print("="*80)
